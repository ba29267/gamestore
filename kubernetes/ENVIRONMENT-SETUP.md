# Environment Setup Guide - Quick Reference

This guide explains how the GameStore application is isolated across different environments and what you need to know about each.

## TL;DR - Quick Start

### Local Development (Docker Compose)

```bash
docker-compose up -d
# Access: http://localhost:8000
# Requires: 6GB+ RAM
```

### Local Development (Kubernetes)

```bash
kubectl apply -k kubernetes/overlays/development
# Requires: 6GB+ RAM, kubectl configured
```

### Staging (Testing)

```bash
kubectl apply -k kubernetes/overlays/staging
# Requires: 12GB+ RAM, multiple replicas enabled
```

### Production (Live)

```bash
kubectl apply -k kubernetes/overlays/production
# Requires: 16GB+ RAM (per node), auto-scaling enabled
```

---

## Environment Comparison Matrix

### Deployment Options

| Metric                   | Docker Compose | Kubernetes Dev | Kubernetes Staging | Kubernetes Prod  |
| ------------------------ | -------------- | -------------- | ------------------ | ---------------- |
| **Setup Time**           | 5 minutes      | 10-15 minutes  | 15-20 minutes      | 30+ minutes      |
| **Complexity**           | Simple         | Medium         | Medium             | Advanced         |
| **Min RAM**              | 6GB            | 6GB            | 12GB               | 16GB+ per node   |
| **Service Replicas**     | 1 each         | 1 each         | 2-3 each           | 4+ each          |
| **Auto-scaling**         | No             | No             | Yes (1-3)          | Yes (1-10+)      |
| **Database Replication** | No             | No             | Optional           | Yes (HA)         |
| **Best For**             | Development    | Testing        | QA/Staging         | Users/Production |
| **Scaling**              | Manual         | Manual         | Semi-auto          | Fully Auto       |

---

## 1. Development Environment (gamestore-dev)

### Use Case

- Local machine development
- Testing code changes
- Debugging issues
- Experimenting with features

### Resource Profile

```
Total Namespace Quota: 8Gi memory, 4 CPU cores
Per-Service: 1 replica
Container Memory Usage: ~2.5GB
Host RAM Required: 6GB (minimum)
Disk Space: 15GB
```

### Key Characteristics

- âœ… Minimal resource usage (1 replica per service)
- âœ… Fast iteration (no waiting for multiple pods)
- âœ… Easy debugging (single pod to observe)
- âŒ No high availability
- âŒ No auto-scaling
- âŒ Single point of failure

### Network Configuration

```
Namespace: gamestore-dev
Services: Used via headless DNS
Example: http://game-service.gamestore-dev.svc.cluster.local:3002
Frontend: http://localhost:3000 (port-forward)
```

### How to Deploy

```bash
# Deploy development environment
kubectl apply -k kubernetes/overlays/development

# Verify
kubectl get all -n gamestore-dev

# Port forward for access
kubectl port-forward -n gamestore-dev svc/frontend 3000:3000

# Access at http://localhost:3000
```

### Single Service Update

```bash
# Modify code in services/game-service/

# Rebuild
docker build -t gamestore-game-service:latest services/game-service

# Load into Minikube
minikube image load gamestore-game-service:latest

# Restart pod
kubectl rollout restart deployment/game-service -n gamestore-dev

# Watch rollout
kubectl get pods -n gamestore-dev --watch

# Check logs
kubectl logs -f deployment/game-service -n gamestore-dev
```

### Data Persistence

```
PostgreSQL: Single instance, 10GB PVC
Redis: Single instance, 5GB PVC (optional)
Solr: Single instance, 10GB PVC (optional)
Data survives pod restarts
```

### When to Use Dev Environment

- âœ… Local development on your machine
- âœ… Quick testing of changes
- âœ… Learning Kubernetes basics
- âœ… Running locally with Docker Desktop
- âŒ Performance testing
- âŒ Load testing
- âŒ Team/QA testing

---

## 2. Staging Environment (gamestore-staging)

### Use Case

- Pre-production testing
- Quality assurance validation
- Performance testing
- Release candidate verification
- Team testing before production

### Resource Profile

```
Total Namespace Quota: 16Gi memory, 8 CPU cores
Per-Service: 2-3 replicas (HPA manages 1-3)
Container Memory Usage: ~5-7GB typical
Host RAM Required: 12GB (minimum)
Disk Space: 30GB
Target Node Count: 2-3
```

### Key Characteristics

- âœ… Multiple replicas (tests scalability)
- âœ… HPA enabled (auto-scaling 1-3 replicas)
- âœ… More realistic load testing
- âœ… Closer to production behavior
- âœ… Better fault tolerance
- âš ï¸ Uses more resources than dev
- âš ï¸ Slower pod startup/shutdown

### Network Configuration

```
Namespace: gamestore-staging
Ingress: Enabled with path-based routing
External Access: http://staging.example.com
TLS: Self-signed or staging CA certificate
Rate Limiting: Moderate (allows testing)
```

### How to Deploy

```bash
# Deploy staging environment
kubectl apply -k kubernetes/overlays/staging

# Verify with multiple replicas
kubectl get pods -n gamestore-staging

# Check HPA status
kubectl get hpa -n gamestore-staging

# Monitor auto-scaling
kubectl get hpa -n gamestore-staging --watch

# View metrics
kubectl get pods -n gamestore-staging -o custom-columns=\
  NAME:.metadata.name,CPU:.usage.cpu,MEMORY:.usage.memory
```

### Load Testing in Staging

```bash
# Test with load generator
# This triggers HPA to scale up to 3 replicas (if using staging)
for i in {1..1000}; do
  curl http://staging.example.com/api/games &
done

# Monitor scaling
kubectl get pods -n gamestore-staging --watch

# Check HPA decisions
kubectl describe hpa game-service-hpa -n gamestore-staging
```

### Database Configuration

```
PostgreSQL:
  - Single primary instance
  - Optional read replica
  - Daily backups
  - 20GB PVC

Redis:
  - Potentially replicated for HA testing
  - 10GB PVC

Solr:
  - Single instance or cluster mode
  - 20GB PVC
```

### When to Use Staging

- âœ… QA/UAT testing
- âœ… Performance validation
- âœ… Scaling testing
- âœ… Release candidate verification
- âœ… Team/Business testing
- âœ… Load testing (moderate)
- âŒ Production traffic
- âŒ User-facing deployment
- âŒ Real payments/transactions

### Pre-Production Checklist

```
â˜ All code changes reviewed and merged
â˜ Automated tests passing
â˜ Staging deployed and stable for 30+ minutes
â˜ Performance meets SLA targets
â˜ No memory leaks or resource issues
â˜ Backup/restore tested
â˜ Logs aggregated and searchable
â˜ Monitoring/alerting validated
â˜ Security scan completed
â˜ QA sign-off obtained
```

---

## 3. Production Environment (gamestore-prod)

### Use Case

- Live user traffic
- Real transactions
- Critical operations
- Maximum reliability required

### Resource Profile

```
Total Namespace: UNLIMITED (auto-scales as needed)
Per-Service: 4+ replicas (HPA manages 1-10+)
Container Memory Usage: 10GB+ (scales dynamically)
Host RAM Required: 16GB+ per node (typically 3+ nodes)
Disk Space: 100GB+ (across multiple nodes)
Target Node Count: 3-5+ (multi-zone for HA)
```

### Key Characteristics

- âœ… High availability (4+ replicas per service)
- âœ… Aggressive auto-scaling (1-10+ replicas based on load)
- âœ… Distributed across multiple nodes
- âœ… VPA enabled for continuous optimization
- âœ… No resource quotas (unlimited scaling)
- âœ… Real-time backups and PITR capability
- âš ï¸ Complex multi-node setup
- âš ï¸ Continuous cost implications
- âš ï¸ Requires operability/monitoring expertise

### Network Configuration

```
Namespace: gamestore-prod
Ingress: Production-grade with WAF
External Access: http://gamestore.com
TLS: Production CA certificate (e.g., LetsEncrypt)
Rate Limiting: Strict per-user/IP
Load Balancer: Cloud provider's LB (AWS/GCP/Azure)
CDN: Optional for static assets
DDoS Protection: Enabled
```

### How to Deploy

```bash
# Deploy production environment
kubectl apply -k kubernetes/overlays/production

# Verify high availability
kubectl get pods -n gamestore-prod
# Should show 4+ replicas per service

# Check HPA aggressive scaling
kubectl get hpa -n gamestore-prod
kubectl describe hpa game-service-hpa -n gamestore-prod

# Monitor with Prometheus
kubectl port-forward -n gamestore-prod svc/prometheus 9090:9090
# Open http://localhost:9090

# View Grafana dashboards
kubectl port-forward -n gamestore-prod svc/grafana 3000:3000
# Open http://localhost:3000
```

### Database Configuration

```
PostgreSQL:
  - Primary + 1+ Read Replicas (HA setup)
  - Synchronous replication
  - Multi-AZ deployment (if on cloud)
  - Real-time backups (PITR enabled)
  - 100GB+ PVC (distributed storage)

Redis:
  - Master + Replica (or Redis Cluster)
  - Persistent AOF enabled
  - 20GB+ PVC

Solr:
  - SolrCloud mode (distributed)
  - Replication factor: 2+
  - 50GB+ PVC (across nodes)
```

### Scaling Behavior

```
GET /api/games (load increases):
  Replicas: 2 â†’ 4 â†’ 6 â†’ 8 â†’ 10
  Trigger: CPU > 70% or Memory > 80%

Cooldown: 300 seconds (5 min)
Scale-up delay: 0 (immediate)
Scale-down delay: 300 seconds (conservative)
Min replicas: 2 (never go below)
Max replicas: 10 (cost control)
```

### Deployment Process (Production)

```bash
# 1. Build and test
docker build -t gamestore-game-service:v1.2.3 services/game-service
docker push gcr.io/my-project/gamestore-game-service:v1.2.3

# 2. Blue-Green Deployment (manual)
# Deploy new version alongside old:
kubectl set image deployment/game-service \
  game-service=gcr.io/my-project/gamestore-game-service:v1.2.3 \
  -n gamestore-prod

# 3. Monitor rollout
kubectl rollout status deployment/game-service -n gamestore-prod -w

# 4. If issues, instant rollback available
kubectl rollout undo deployment/game-service -n gamestore-prod
```

### Monitoring & Alerting

```
Prometheus: 90-day retention
Grafana: Real-time dashboards
Alerts: PagerDuty, Slack, Email
Log Aggregation: ELK or Loki
APM: Datadog or similar (optional)

Key Metrics:
- Request latency (p50, p95, p99)
- Error rate (4xx, 5xx)
- Pod restart count
- Memory usage trends
- Database connection pool
- Cache hit ratio
```

### When to Use Production

- âœ… User-facing applications
- âœ… Real transactions/payments
- âœ… Need 99.9%+ uptime
- âœ… Scaling requirements unpredictable
- âœ… Data loss is unacceptable
- âŒ Testing/development
- âŒ Cost-sensitive operations
- âŒ First-time Kubernetes users

### Production Checklist

```
â˜ All staging checks passed
â˜ Production secrets configured
â˜ Backups tested and verified
â˜ Monitoring and alerting active
â˜ On-call team ready
â˜ Disaster recovery plan documented
â˜ Team trained on incident response
â˜ Database HA configured
â˜ SSL certificates valid
â˜ DNS records updated
â˜ CDN configured (if applicable)
â˜ Rate limiting configured
â˜ WAF rules enabled
â˜ DDoS protection active
â˜ Audit logging enabled
```

---

## Resource Requirements Summary

### Host Machine Sizing

```
4GB RAM: âŒ NOT RECOMMENDED (containers crash frequently)
6GB RAM: âœ… For Docker Compose or Kubernetes Dev
8GB RAM: âœ… Safe for local development
12GB RAM: âœ… For staging on local machine
16GB+ RAM: âœ… For production-like testing

Disk Space:
- 10GB: Minimum (with aggressive cleanup)
- 20GB: Comfortable for dev/staging
- 50GB+: Production installations
```

### Container Memory Breakdown

```
PostgreSQL:        256-512 MB
Redis:             100-256 MB
Solr:              512 MB (Java)
4 Microservices:   600-800 MB total
Frontend:          100-150 MB
Nginx:             64-128 MB
Prometheus:        256-512 MB
Grafana:           200-300 MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Subtotal:          ~2.5-3.5 GB

System Overhead:
Docker daemon:     300-500 MB
OS/Kubelet:        1.5-2.0 GB
Buffers/Cache:     500-1000 MB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
System Total:      ~2-3 GB

TOTAL REQUIRED:    ~5-6+ GB (6GB minimum recommended)
```

---

## Switching Between Environments

### From Dev to Staging

```bash
# Switch namespace context
kubectl config set-context --current --namespace=gamestore-staging

# Or specify namespace explicitly
kubectl get pods -n gamestore-staging

# New kubeconfig for easy switching
kubectl config rename-context docker-desktop dev
kubectl config rename-context staging-context staging
kubectl config rename-context prod-context prod

# Switch
kubectl config use-context staging
```

### Comparing Environment Configs

```bash
# Show dev resources
kubectl get deployments -n gamestore-dev -o wide

# Show staging (higher replicas)
kubectl get deployments -n gamestore-staging -o wide

# Show prod (even higher replicas, auto-scaling)
kubectl get deployments -n gamestore-prod -o wide
```

---

## Environment Isolation: How It Works

### Namespace Isolation

```
gamestore-dev/
  â”œâ”€ Separate from other envs
  â”œâ”€ Own database (postgres-dev)
  â”œâ”€ Own Redis cache
  â””â”€ Resource quota: 8Gi

gamestore-staging/
  â”œâ”€ Separate from other envs
  â”œâ”€ Own database (postgres-staging)
  â”œâ”€ Own Redis cache
  â””â”€ Resource quota: 16Gi

gamestore-prod/
  â”œâ”€ Separate from other envs
  â”œâ”€ Own database (postgres-prod)
  â”œâ”€ Own Redis cache
  â””â”€ Resource quota: Unlimited
```

### ConfigMap/Secrets by Environment

```
dev:
  - LOG_LEVEL: debug
  - NODE_ENV: development
  - DB_HOST: postgres.gamestore-dev.svc.cluster.local

staging:
  - LOG_LEVEL: info
  - NODE_ENV: staging
  - DB_HOST: postgres.gamestore-staging.svc.cluster.local

prod:
  - LOG_LEVEL: warn
  - NODE_ENV: production
  - DB_HOST: postgres.gamestore-prod.svc.cluster.local
```

### Kustomize Magic

```
kubernetes/base/               # Shared configuration
â”œâ”€ auth-service.yaml
â”œâ”€ game-service.yaml
â”œâ”€ postgres.yaml
â””â”€ kustomization.yaml          # References base

kubernetes/overlays/development/
â”œâ”€ kustomization.yaml          # Patches base for dev
â”œâ”€ Reduces replicas to 1
â”œâ”€ Sets imagePolicy: IfNotPresent
â”œâ”€ Limits resources
â””â”€ Namespace: gamestore-dev

kubernetes/overlays/staging/
â”œâ”€ kustomization.yaml          # Patches base for staging
â”œâ”€ Increases replicas to 2-3
â”œâ”€ Enables HPA
â”œâ”€ Moderate resource limits
â””â”€ Namespace: gamestore-staging

kubernetes/overlays/production/
â”œâ”€ kustomization.yaml          # Patches base for prod
â”œâ”€ Increases replicas to 4+
â”œâ”€ Enables aggressive auto-scaling
â”œâ”€ High resource limits
â””â”€ Namespace: gamestore-prod
```

---

## Troubleshooting: 4GB RAM Issue

### Problem: "Out of Memory" crashes

**Symptoms:**

- Pods restart frequently
- `kubectl describe pod` shows `OOMKilled`
- Services unreachable intermittently
- Application logs show memory errors

**Root Cause:**

- Host has only 4GB RAM
- Containers (2.5GB) + System overhead (2.5GB) = 5GB total
- No buffer for spikes â†’ OOMKilled

**Solutions (in order of preference):**

1. **Upgrade to 6GB+ RAM** (recommended)
   - Most reliable solution
   - Eliminates memory contention
   - Supports all features

2. **Use Docker Compose instead**

   ```bash
   docker-compose up -d
   # Lighter than Kubernetes (~500MB less overhead)
   ```

3. **Disable monitoring**

   ```yaml
   # Edit kubernetes/base/kustomization.yaml
   # Comment out:
   # - monitoring.yaml     # Saves ~450MB
   ```

4. **Disable Solr search**

   ```yaml
   # Comment:
   # - solr.yaml          # Saves ~512MB
   # Not needed for basic testing
   ```

5. **Reduce resource limits** (last resort)
   ```yaml
   # Edit services, change:
   requests:
     memory: "64Mi" # (was 128Mi)
     cpu: "50m" # (was 100m)
   limits:
     memory: "128Mi" # (was 256Mi)
     cpu: "100m" # (was 200m)
   ```

### Verify Current Memory Usage

```bash
# Using Docker
docker stats --no-stream

# Using Kubernetes
kubectl top nodes              # Node memory usage
kubectl top pods -n gamestore-dev   # Pod memory usage
```

---

## Additional Resources

- ğŸ“– [Full Architecture Guide](ARCHITECTURE.md)
- ğŸ“Š [Component Diagram](COMPONENT-DIAGRAM.md)
- ğŸ“˜ [Kubernetes README](README.md)
- ğŸ”— [Kustomize Documentation](https://kustomize.io/)
- âš™ï¸ [kubectl Cheat Sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet/)

---

**Last Updated:** February 2024  
**Status:** Complete & Tested
